{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyN3IaHrA8r0gSSjUr4QZxVV"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["**ASSIGNMENT QUESTIONS**"],"metadata":{"id":"2cX_jl156d-w"}},{"cell_type":"markdown","source":["1. What is Simple Linear Regression?\n","- Simple Linear Regression is a method used to model the relationship between two variables by fitting a linear equation to the observed data. One variable is considered independent (predictor) and the other dependent (response).\n","\n","2. What are the key assumptions of Simple Linear Regression?\n","\n","- Linearity: Relationship between x and y is linear.\n","\n","- Independence: Observations are independent.\n","\n","- Homoscedasticity: Constant variance of errors.\n","\n","- Normality: Errors are normally distributed.\n","3. What does the coefficient m represent in the equation Y = mX + c?\n","- The coefficient m represents the slope of the line, indicating the change in the dependent variable (Y) for a one-unit increase in the independent variable (X).\n","\n","4. What does the intercept c represent in the equation Y = mX + c?\n","- The intercept c is the value of Y when X is zero; it represents the point where the line crosses the Y-axis.\n","\n","5. How do we calculate the slope m in Simple Linear Regression?\n","- The slope m is calculated as:\n","𝑚\n","=\n","Cov\n","(\n","𝑋\n",",\n","𝑌\n",")\n","Var\n","(\n","𝑋\n",")\n","m=\n","Var(X)\n","Cov(X,Y)\n","​\n","\n","  where Cov(X, Y) is the covariance of X and Y, and Var(X) is the variance of X.\n","\n","6. What is the purpose of the least squares method in Simple Linear Regression?\n","- The least squares method minimizes the sum of the squares of the residuals (errors) between observed and predicted values, providing the best-fitting line.\n","\n","7. How is the coefficient of determination (R²) interpreted in Simple Linear Regression?\n","- R² measures the proportion of variance in the dependent variable that can be explained by the independent variable. It ranges from 0 to 1.\n","\n","8. What is Multiple Linear Regression?\n","- Multiple Linear Regression models the relationship between one dependent variable and two or more independent variables using a linear equation.\n","\n","9. What is the main difference between Simple and Multiple Linear Regression?\n","- Simple Linear Regression involves one independent variable, while Multiple Linear Regression involves two or more independent variables.\n","\n","10. What are the key assumptions of Multiple Linear Regression?\n","\n","- Linearity between dependent and independent variables.\n","\n","- Independence of errors.\n","\n","- Homoscedasticity.\n","\n","- No multicollinearity among independent variables.\n","\n","- Normal distribution of errors.\n","\n","11. What is heteroscedasticity, and how does it affect the results of a Multiple Linear Regression model?\n","- Heteroscedasticity means the variance of errors is not constant. It can lead to inefficient estimates and biased standard errors, affecting hypothesis testing.\n","\n","12. How can you improve a Multiple Linear Regression model with high multicollinearity?\n","\n","- Remove highly correlated variables.\n","\n","- Use dimensionality reduction techniques like PCA.\n","\n","- Apply regularization methods like Ridge or Lasso regression.\n","\n","13. What are some common techniques for transforming categorical variables for use in regression models?\n","\n","- One-Hot Encoding\n","\n","- Label Encoding\n","\n","- Binary Encoding\n","\n","- Ordinal Encoding (for ordered categories)\n","\n","14. What is the role of interaction terms in Multiple Linear Regression?\n","- Interaction terms capture the combined effect of two or more variables on the dependent variable, which cannot be explained by their individual effects.\n","\n","15. How can the interpretation of intercept differ between Simple and Multiple Linear Regression?\n","- In Simple Linear Regression, the intercept is the expected value of Y when X is zero. In Multiple Regression, it's the expected Y when all independent variables are zero.\n","\n","16. What is the significance of the slope in regression analysis, and how does it affect predictions?\n","- The slope indicates the magnitude and direction of the relationship between an independent variable and the dependent variable; it helps predict Y given X.\n","\n","17. How does the intercept in a regression model provide context for the relationship between variables?\n","- The intercept sets the baseline value of the dependent variable when all predictors are at zero, anchoring the regression line.\n","\n","18. What are the limitations of using R² as a sole measure of model performance?\n","\n","- R² does not indicate if the model is appropriate.\n","\n","- A high R² does not imply causation.\n","\n","- It can increase with more variables even if they are irrelevant.\n","\n","19. How would you interpret a large standard error for a regression coefficient?\n","- A large standard error suggests that the estimate of the coefficient is not precise and may vary significantly from sample to sample.\n","\n","20. How can heteroscedasticity be identified in residual plots, and why is it important to address it?\n","- Heteroscedasticity appears as a funnel or fan shape in residual plots. Addressing it is crucial to ensure valid statistical inference and reliable model estimates.\n","\n","21. What does it mean if a Multiple Linear Regression model has a high R² but low adjusted R²?\n","- It means that some independent variables might not be significantly contributing to the model; adjusted R² accounts for the number of predictors and penalizes overfitting.\n","\n","22. Why is it important to scale variables in Multiple Linear Regression?\n","- Scaling ensures that variables are on the same scale, improving the interpretability and numerical stability of the model, especially when using regularization.\n","\n","23. What is polynomial regression?\n","- Polynomial Regression is an extension of linear regression where the relationship between independent and dependent variables is modeled as an nth-degree polynomial.\n","\n","24. How does polynomial regression differ from linear regression?\n","- While linear regression models straight-line relationships, polynomial regression can model curves by including higher-degree terms (like\n","𝑥\n","2\n","x\n","2\n"," ,\n","𝑥\n","3\n","x\n","3\n"," , etc.).\n","\n","25. When is polynomial regression used?\n","- Polynomial regression is used when data shows a non-linear relationship but can be approximated by a polynomial function.\n","\n","26. What is the general equation for polynomial regression?\n","\n","  𝑌\n","  =\n","  𝑏\n","  0\n","  +\n","  𝑏\n","  1\n","  𝑋\n","  +\n","  𝑏\n","  2\n","  𝑋\n","  2\n","  +\n","  𝑏\n","  3\n","  𝑋\n","  3\n","  +\n","  ⋯\n","  +\n","  𝑏\n","  𝑛\n","  𝑋\n","  𝑛\n","  Y=b\n","  0\n","  ​\n","  +b\n","  1\n","  ​\n","  X+b\n","  2\n","  ​\n","  X\n","  2\n","  +b\n","  3\n","  ​\n","  X\n","  3\n","  +⋯+b\n","  n\n","  ​\n","  X\n","  n\n","\n","  where n is the degree of the polynomial.\n","\n","27. Can polynomial regression be applied to multiple variables?\n","- Yes, it can be extended to multiple variables by including interaction terms and powers of each independent variable.\n","\n","28. What are the limitations of polynomial regression?\n","\n","- Risk of overfitting with high-degree polynomials.\n","\n","- Complex models become harder to interpret.\n","\n","- Poor extrapolation outside the data range.\n","\n","29. What methods can be used to evaluate model fit when selecting the degree of a polynomial?\n","\n","- Cross-validation\n","\n","- Adjusted R²\n","\n","- AIC (Akaike Information Criterion) / BIC (Bayesian Information Criterion)\n","\n","30. Why is visualization important in polynomial regression?\n","- Visualization helps detect overfitting/underfitting and understand how well the polynomial curve fits the underlying data.\n","\n","31. How is polynomial regression implemented in Python?\n","- Polynomial regression can be implemented using libraries like scikit-learn, with PolynomialFeatures to generate polynomial terms and LinearRegression to fit the model."],"metadata":{"id":"K7iXm-084g0u"}}]}